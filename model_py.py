# -*- coding: utf-8 -*-
"""model.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lgseC-IcFpRRAjf7b12pv9NMivCGS_sT
"""

import pandas as pd
MyData = pd.read_excel("Income_Expense_Data.xlsx")

#Checking Size of data
MyData.shape

#Checking first few records
MyData.head(5)

#Check for missing values
MyData.isnull().sum()

#Treating null value-replacing null value with median
MyData["Expense"].fillna((MyData["Expense"].median()), inplace = True)

#Check for missing values - Again
MyData.isnull().sum()

#Checking for outliers
MyData.describe()  #notice the maximum value in Age

#Checking different percentiles
pd.DataFrame(MyData['Age']).describe(percentiles=(1,0.99,0.9,0.75,0.5,0.3,0.1,0.01))

# Commented out IPython magic to ensure Python compatibility.
#checking boxplot for Age column
import matplotlib.pyplot as plt
# %matplotlib inline
plt.boxplot(MyData['Age'])
plt.show()

#Checking Outlier by definition and treating outliers

#getting median Age
Age_col_df = pd.DataFrame(MyData['Age'])
Age_median = Age_col_df.median()

#getting IQR of Age column
Q3 = Age_col_df.quantile(q=0.75)
Q1 = Age_col_df.quantile(q=0.25)
IQR = Q3-Q1

#Deriving boundaries of Outliers
IQR_LL = int(Q1 - 1.5*IQR)
IQR_UL = int(Q3 + 1.5*IQR)

#Finding and treating outliers - both lower and upper end
MyData.loc[MyData['Age']>IQR_UL , 'Age'] = int(Age_col_df.quantile(q=0.90))
MyData.loc[MyData['Age']<IQR_LL , 'Age'] = int(Age_col_df.quantile(q=0.01))

#Check max age value now
max(MyData['Age'])

#Check how Expense is varying with Age
x = MyData["Age"]
y=  MyData["Expense"]
plt.scatter(x, y)
plt.title('Age Income chart')
plt.xlabel('abc')
plt.ylabel('Income')
plt.show()

#check correltion matrix - to check the strength of variation bwtween two variables
correlation_matrix= MyData.corr().round(2)
fgr, ax = plt.subplots(figsize =(8, 4)) 
import seaborn as sns
c = sns.heatmap(data=correlation_matrix, annot=True)

#Normalization/scaling of data - understanding scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(MyData)
scaled_data

#converting data back to pandas dataframe
MyData_scaled = pd.DataFrame(scaled_data)
MyData_scaled.columns = MyData.columns

#Separating features and response
features = ["Income","Age"]
response = ["Expense"]
X=MyData_scaled[features]
y=MyData_scaled[response]

#Dividing data in test and train
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)

#Importing neccesary packages
from sklearn.linear_model import LinearRegression
from sklearn import metrics

#Fitting lineaar regression model
model = LinearRegression()
model.fit(X_train, y_train)

#Checking accuracy on test data
accuracy = model.score(X_test,y_test)
print(accuracy*100,'%')

y_pred = model.predict(X_test)

metrics.r2_score(y_pred,y_test)

X_test #the test data - predictors

y_test #the actual values in test data - target column

y_pred

model.predict(X_test) #predcited values on test data

model.intercept_ #checking the intercept of model equation

model.coef_ #Checking the coefficients of model equation

import numpy as np
json={'Income':50000, 'Age':30}
data = json
prediction = model.predict([np.array(list(data.values()))])

output = prediction[0]
prediction

import pickle
pickle.dump(model, open('model.pkl','wb'))

model = pickle.load(open('model.pkl','rb'))
print(model.predict([[50000, 30]]))
